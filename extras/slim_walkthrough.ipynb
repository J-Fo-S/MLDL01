{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-Slim Walkthrough\n",
    "\n",
    "This notebook will walk you through the basics of using TF-Slim to define, train and evaluate neural networks on various tasks. It assumes a basic knowledge of neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "<a href=\"#Install\">Installation and setup</a><br>\n",
    "<a href='#MLP'>Creating your first neural network with TF-Slim</a><br>\n",
    "<a href='#ReadingTFSlimDatasets'>Reading Data with TF-Slim</a><br>\n",
    "<a href='#CNN'>Training a convolutional neural network (CNN)</a><br>\n",
    "<a href='#Pretained'>Using pre-trained models</a><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and setup\n",
    "<a id='Install'></a>\n",
    "\n",
    "Since the stable release of TF 1.0, the latest version of slim has been available as `tf.contrib.slim`.\n",
    "To test that your installation is working, execute the following command; it should run without raising any errors.\n",
    "\n",
    "```\n",
    "python -c \"import tensorflow.contrib.slim as slim; eval = slim.evaluation.evaluate_once\"\n",
    "```\n",
    "\n",
    "Although, to use TF-Slim for image classification (as we do in this notebook), you also have to install the TF-Slim image models library from [here](https://github.com/tensorflow/models/tree/master/research/slim). Let's suppose you install this into a directory called TF_MODELS. Then you should change directory to  TF_MODELS/research/slim **before** running this notebook, so that these files are in your python path.\n",
    "\n",
    "To check you've got these two steps to work, just execute the cell below. If it complains about unknown modules, restart the notebook after moving to the TF-Slim models directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathansherman/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from datasets import dataset_utils\n",
    "\n",
    "# Main slim library\n",
    "from tensorflow.contrib import slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your first neural network with TF-Slim\n",
    "<a id='MLP'></a>\n",
    "\n",
    "Below we give some code to create a simple multilayer perceptron (MLP)  which can be used\n",
    "for regression problems. The model has 2 hidden layers.\n",
    "The output is a single node. \n",
    "When this function is called, it will create various nodes, and silently add them to whichever global TF graph is currently in scope. When a node which corresponds to a layer with adjustable parameters (eg., a fully connected layer) is created, additional parameter variable nodes are silently created, and added to the graph. (We will discuss how to train the parameters later.)\n",
    "\n",
    "We use variable scope to put all the nodes under a common name,\n",
    "so that the graph has some hierarchical structure.\n",
    "This is useful when we want to visualize the TF graph in tensorboard, or if we want to query related\n",
    "variables. \n",
    "The fully connected layers all use the same L2 weight decay and ReLu activations, as specified by **arg_scope**. (However, the final layer overrides these defaults, and uses an identity activation function.)\n",
    "\n",
    "We also illustrate how to add a dropout layer after the first fully connected layer (FC1). Note that at test time, \n",
    "we do not drop out nodes, but instead use the average activations; hence we need to know whether the model is being\n",
    "constructed for training or testing, since the computational graph will be different in the two cases\n",
    "(although the variables, storing the model parameters, will be shared, since they have the same name/scope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression_model(inputs, is_training=True, scope=\"deep_regression\"):\n",
    "    \"\"\"Creates the regression model.\n",
    "\n",
    "    Args:\n",
    "        inputs: A node that yields a `Tensor` of size [batch_size, dimensions].\n",
    "        is_training: Whether or not we're currently training the model.\n",
    "        scope: An optional variable_op scope for the model.\n",
    "\n",
    "    Returns:\n",
    "        predictions: 1-D `Tensor` of shape [batch_size] of responses.\n",
    "        end_points: A dict of end points representing the hidden layers.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, 'deep_regression', [inputs]):\n",
    "        end_points = {}\n",
    "        # Set the default weight _regularizer and acvitation for each fully_connected layer.\n",
    "        with slim.arg_scope([slim.fully_connected],\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            weights_regularizer=slim.l2_regularizer(0.01)):\n",
    "\n",
    "            # Creates a fully connected layer from the inputs with 32 hidden units.\n",
    "            net = slim.fully_connected(inputs, 32, scope='fc1')\n",
    "            end_points['fc1'] = net\n",
    "\n",
    "            # Adds a dropout layer to prevent over-fitting.\n",
    "            net = slim.dropout(net, 0.8, is_training=is_training)\n",
    "\n",
    "            # Adds another fully connected layer with 16 hidden units.\n",
    "            net = slim.fully_connected(net, 16, scope='fc2')\n",
    "            end_points['fc2'] = net\n",
    "\n",
    "            # Creates a fully-connected layer with a single hidden unit. Note that the\n",
    "            # layer is made linear by setting activation_fn=None.\n",
    "            predictions = slim.fully_connected(net, 1, activation_fn=None, scope='prediction')\n",
    "            end_points['out'] = predictions\n",
    "\n",
    "            return predictions, end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create the model and examine its structure.\n",
    "\n",
    "We create a TF graph and call regression_model(), which adds nodes (tensors) to the graph. We then examine their shape, and print the names of all the model variables which have been implicitly created inside of each layer. We see that the names of the variables follow the scopes that we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers\n",
      "name = deep_regression/fc1/Relu:0, shape = (?, 32)\n",
      "name = deep_regression/prediction/BiasAdd:0, shape = (?, 1)\n",
      "name = deep_regression/fc2/Relu:0, shape = (?, 16)\n",
      "\n",
      "\n",
      "Parameters\n",
      "name = deep_regression/fc1/weights:0, shape = (1, 32)\n",
      "name = deep_regression/fc1/biases:0, shape = (32,)\n",
      "name = deep_regression/fc2/weights:0, shape = (32, 16)\n",
      "name = deep_regression/fc2/biases:0, shape = (16,)\n",
      "name = deep_regression/prediction/weights:0, shape = (16, 1)\n",
      "name = deep_regression/prediction/biases:0, shape = (1,)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    # Dummy placeholders for arbitrary number of 1d inputs and outputs\n",
    "    inputs = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    outputs = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "    # Build model\n",
    "    predictions, end_points = regression_model(inputs)\n",
    "\n",
    "    # Print name and shape of each tensor.\n",
    "    print(\"Layers\")\n",
    "    for k, v in end_points.items():\n",
    "        print('name = {}, shape = {}'.format(v.name, v.get_shape()))\n",
    "\n",
    "    # Print name and shape of parameter nodes  (values not yet initialized)\n",
    "    print(\"\\n\")\n",
    "    print(\"Parameters\")\n",
    "    for v in slim.get_model_variables():\n",
    "        print('name = {}, shape = {}'.format(v.name, v.get_shape()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create some 1d regression data .\n",
    "\n",
    "We will train and test the model on some noisy observations of a nonlinear function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1280456d8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def produce_batch(batch_size, noise=0.3):\n",
    "    xs = np.random.random(size=[batch_size, 1]) * 10\n",
    "    ys = np.sin(xs) + 5 + np.random.normal(size=[batch_size, 1], scale=noise)\n",
    "    return [xs.astype(np.float32), ys.astype(np.float32)]\n",
    "\n",
    "x_train, y_train = produce_batch(200)\n",
    "x_test, y_test = produce_batch(200)\n",
    "plt.scatter(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's fit the model to the data\n",
    "\n",
    "The user has to specify the loss function and the optimizer, and slim does the rest.\n",
    "In particular,  the slim.learning.train function does the following:\n",
    "\n",
    "- For each iteration, evaluate the train_op, which updates the parameters using the optimizer applied to the current minibatch. Also, update the global_step.\n",
    "- Occasionally store the model checkpoint in the specified directory. This is useful in case your machine crashes  - then you can simply restart from the specified checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_data_to_tensors(x, y):\n",
    "    inputs = tf.constant(x)\n",
    "    inputs.set_shape([None, 1])\n",
    "    \n",
    "    outputs = tf.constant(y)\n",
    "    outputs.set_shape([None, 1])\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-2fe465e1f7b8>:16: get_total_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_total_loss instead.\n",
      "WARNING:tensorflow:From /Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:257: get_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_losses instead.\n",
      "WARNING:tensorflow:From /Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:259: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_regularization_losses instead.\n",
      "WARNING:tensorflow:From /Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py:736: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.MonitoredTrainingSession\n",
      "INFO:tensorflow:Restoring parameters from /tmp/regression_model/model.ckpt\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.NotFoundError'>, Key beta1_power not found in checkpoint\n",
      "\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n",
      "Caused by op 'save/RestoreV2', defined at:\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n",
      "    ioloop.IOLoop.instance().start()\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-6-2fe465e1f7b8>\", line 28, in <module>\n",
      "    log_every_n_steps=500)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 658, in train\n",
      "    saver = saver or tf_saver.Saver()\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n",
      "    self.build()\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n",
      "    self._build(self._filename, build_save=True, build_restore=True)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n",
      "    build_save=build_save, build_restore=build_restore)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n",
      "    restore_sequentially, reshape)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n",
      "    restore_sequentially)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n",
      "    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n",
      "    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n",
      "    op_def=op_def)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n",
      "    op_def=op_def)\n",
      "  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n",
      "    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n",
      "\n",
      "NotFoundError (see above for traceback): Key beta1_power not found in checkpoint\n",
      "\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
      "\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-2fe465e1f7b8>\", line 28, in <module>\n    log_every_n_steps=500)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 658, in train\n    saver = saver or tf_saver.Saver()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-2fe465e1f7b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mnumber_of_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msave_summaries_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         log_every_n_steps=500)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Finished training. Last batch loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_op, logdir, train_step_fn, train_step_kwargs, log_every_n_steps, graph, master, is_chief, global_step, number_of_steps, init_op, init_feed_dict, local_init_op, init_fn, ready_op, summary_op, save_summaries_secs, summary_writer, startup_delay_steps, saver, save_interval_secs, sync_optimizer, session_config, session_wrapper, trace_every_n_steps, ignore_live_threads)\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0mshould_retry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m       with sv.managed_session(\n\u001b[0;32m--> 747\u001b[0;31m           master, start_standard_services=False, config=session_config) as sess:\n\u001b[0m\u001b[1;32m    748\u001b[0m         \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msession_wrapper\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;31m# threads which are not checking for `should_stop()`.  They\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m         \u001b[0;31m# will be stopped when we close the session further down.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose_summary_writer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;31m# Close the session to finish up all pending calls.  We do not care\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, threads, close_summary_writer, ignore_live_threads)\u001b[0m\n\u001b[1;32m    826\u001b[0m           \u001b[0mthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m           \u001b[0mstop_grace_period_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_grace_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m           ignore_live_threads=ignore_live_threads)\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0;31m# Close the writer last, in case one of the running threads was using it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    387\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_registered_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exc_info_to_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m       \u001b[0;32melif\u001b[0m \u001b[0mstragglers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_live_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mmanaged_session\u001b[0;34m(self, master, config, start_standard_services, close_summary_writer)\u001b[0m\n\u001b[1;32m    987\u001b[0m       sess = self.prepare_or_wait_for_session(\n\u001b[1;32m    988\u001b[0m           \u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m           start_standard_services=start_standard_services)\n\u001b[0m\u001b[1;32m    990\u001b[0m       \u001b[0;32myield\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/supervisor.py\u001b[0m in \u001b[0;36mprepare_or_wait_for_session\u001b[0;34m(self, master, config, wait_for_checkpoint, max_wait_secs, start_standard_services)\u001b[0m\n\u001b[1;32m    724\u001b[0m           \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m           \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m           init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)\n\u001b[0m\u001b[1;32m    727\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mstart_standard_services\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36mprepare_session\u001b[0;34m(self, master, init_op, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config, init_feed_dict, init_fn)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_for_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_wait_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         config=config)\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_loaded_from_checkpoint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minit_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minit_fn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_local_init_op\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/session_manager.py\u001b[0m in \u001b[0;36m_restore_checkpoint\u001b[0;34m(self, master, saver, checkpoint_dir, checkpoint_filename_with_path, wait_for_checkpoint, max_wait_secs, config)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;31m# Loads the checkpoint.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecover_last_checkpoints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_model_checkpoint_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1800\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m       sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1802\u001b[0;31m                {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2', defined at:\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-6-2fe465e1f7b8>\", line 28, in <module>\n    log_every_n_steps=500)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 658, in train\n    saver = saver or tf_saver.Saver()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1338, in __init__\n    self.build()\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1347, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1384, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 835, in _build_internal\n    restore_sequentially, reshape)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 472, in _AddRestoreOps\n    restore_sequentially)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 886, in bulk_restore\n    return io_ops.restore_v2(filename_tensor, names, slices, dtypes)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1463, in restore_v2\n    shape_and_slices=shape_and_slices, dtypes=dtypes, name=name)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Key beta1_power not found in checkpoint\n\t [[Node: save/RestoreV2 = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save/Const_0_0, save/RestoreV2/tensor_names, save/RestoreV2/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "# The following snippet trains the regression model using a mean_squared_error loss.\n",
    "ckpt_dir = '/tmp/regression_model/'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    inputs, targets = convert_data_to_tensors(x_train, y_train)\n",
    "\n",
    "    # Make the model.\n",
    "    predictions, nodes = regression_model(inputs, is_training=True)\n",
    "\n",
    "    # Add the loss function to the graph.\n",
    "    loss = tf.losses.mean_squared_error(labels=targets, predictions=predictions)\n",
    "    \n",
    "    # The total loss is the user's loss plus any regularization losses.\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.005)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer) \n",
    "\n",
    "    # Run the training inside a session.\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=ckpt_dir,\n",
    "        number_of_steps=5000,\n",
    "        save_summaries_secs=5,\n",
    "        log_every_n_steps=500)\n",
    "  \n",
    "print(\"Finished training. Last batch loss:\", final_loss)\n",
    "print(\"Checkpoint saved in %s\" % ckpt_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with multiple loss functions.\n",
    "\n",
    "Sometimes we have multiple objectives we want to simultaneously optimize.\n",
    "In slim, it is easy to add more losses, as we show below. (We do not optimize the total loss in this example,\n",
    "but we show how to compute it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-3f76089a1a21>:7: absolute_difference (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.absolute_difference instead.\n",
      "WARNING:tensorflow:From /Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:295: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:146: add_arg_scope.<locals>.func_with_args (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "Total Loss1: 37.135422\n",
      "Total Loss2: 37.135422\n",
      "Regularization Losses:\n",
      "Tensor(\"deep_regression/fc1/kernel/Regularizer/l2_regularizer:0\", shape=(), dtype=float32)\n",
      "Tensor(\"deep_regression/fc2/kernel/Regularizer/l2_regularizer:0\", shape=(), dtype=float32)\n",
      "Tensor(\"deep_regression/prediction/kernel/Regularizer/l2_regularizer:0\", shape=(), dtype=float32)\n",
      "Loss Functions:\n",
      "Tensor(\"mean_squared_error/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"absolute_difference/value:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    inputs, targets = convert_data_to_tensors(x_train, y_train)\n",
    "    predictions, end_points = regression_model(inputs, is_training=True)\n",
    "\n",
    "    # Add multiple loss nodes.\n",
    "    mean_squared_error_loss = tf.losses.mean_squared_error(labels=targets, predictions=predictions)\n",
    "    absolute_difference_loss = slim.losses.absolute_difference(predictions, targets)\n",
    "\n",
    "    # The following two ways to compute the total loss are equivalent\n",
    "    regularization_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "    total_loss1 = mean_squared_error_loss + absolute_difference_loss + regularization_loss\n",
    "\n",
    "    # Regularization Loss is included in the total loss by default.\n",
    "    # This is good for training, but not for testing.\n",
    "    total_loss2 = slim.losses.get_total_loss(add_regularization_losses=True)\n",
    "    \n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op) # Will initialize the parameters with random weights.\n",
    "        \n",
    "        total_loss1, total_loss2 = sess.run([total_loss1, total_loss2])\n",
    "        \n",
    "        print('Total Loss1: %f' % total_loss1)\n",
    "        print('Total Loss2: %f' % total_loss2)\n",
    "\n",
    "        print('Regularization Losses:')\n",
    "        for loss in slim.losses.get_regularization_losses():\n",
    "            print(loss)\n",
    "\n",
    "        print('Loss Functions:')\n",
    "        for loss in slim.losses.get_losses():\n",
    "            print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the saved model and use it for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/regression_model/model.ckpt\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path /tmp/regression_model/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'red=true, blue=predicted')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    inputs, targets = convert_data_to_tensors(x_test, y_test)\n",
    "  \n",
    "    # Create the model structure. (Parameters will be loaded below.)\n",
    "    predictions, end_points = regression_model(inputs, is_training=False)\n",
    "\n",
    "    # Make a session which restores the old parameters from a checkpoint.\n",
    "    sv = tf.train.Supervisor(logdir=ckpt_dir)\n",
    "    with sv.managed_session() as sess:\n",
    "        inputs, predictions, targets = sess.run([inputs, predictions, targets])\n",
    "\n",
    "plt.scatter(inputs, targets, c='r');\n",
    "plt.scatter(inputs, predictions, c='b');\n",
    "plt.title('red=true, blue=predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compute various evaluation metrics on the test set.\n",
    "\n",
    "In TF-Slim termiology, losses are optimized, but metrics (which may not be differentiable, e.g., precision and recall) are just measured. As an illustration, the code below computes mean squared error and mean absolute error metrics on the test set.\n",
    "\n",
    "Each metric declaration creates several local variables (which must be initialized via tf.initialize_local_variables()) and returns both a value_op and an update_op. When evaluated, the value_op returns the current value of the metric. The update_op loads a new batch of data, runs the model, obtains the predictions and accumulates the metric statistics appropriately before returning the current value of the metric. We store these value nodes and update nodes in 2 dictionaries.\n",
    "\n",
    "After creating the metric nodes, we can pass them to slim.evaluation.evaluation, which repeatedly evaluates these nodes the specified number of times. (This allows us to compute the evaluation in a streaming fashion across minibatches, which is usefulf for large datasets.) Finally, we print the final value of each metric.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-33510a38425e>:8: streaming_mean_absolute_error (from tensorflow.contrib.metrics.python.ops.metric_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.metrics.mean.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/regression_model/model.ckpt\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Starting standard services.\n",
      "INFO:tensorflow:Saving checkpoint to path /tmp/regression_model/model.ckpt\n",
      "INFO:tensorflow:Starting queue runners.\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'TypeError'>, 'module' object is not callable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-33510a38425e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mnum_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Single pass over data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0meval_op\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames_to_update_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             final_op=names_to_value_nodes.values())\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnames_to_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_value_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    inputs, targets = convert_data_to_tensors(x_test, y_test)\n",
    "    predictions, end_points = regression_model(inputs, is_training=False)\n",
    "\n",
    "    # Specify metrics to evaluate:\n",
    "    names_to_value_nodes, names_to_update_nodes = slim.metrics.aggregate_metric_map({\n",
    "      'Mean Squared Error': slim.metrics.streaming_mean_squared_error(predictions, targets),\n",
    "      'Mean Absolute Error': slim.metrics.streaming_mean_absolute_error(predictions, targets)\n",
    "    })\n",
    "\n",
    "    # Make a session which restores the old graph parameters, and then run eval.\n",
    "    sv = tf.train.Supervisor(logdir=ckpt_dir)\n",
    "    with sv.managed_session() as sess:\n",
    "        metric_values = slim.evaluation.evaluation(\n",
    "            sess,\n",
    "            num_evals=1, # Single pass over data\n",
    "            eval_op=names_to_update_nodes.values(),\n",
    "            final_op=names_to_value_nodes.values())\n",
    "\n",
    "    names_to_values = dict(zip(names_to_value_nodes.keys(), metric_values))\n",
    "    for key, value in names_to_values.items():\n",
    "      print('%s: %f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data with TF-Slim\n",
    "<a id='ReadingTFSlimDatasets'></a>\n",
    "\n",
    "Reading data with TF-Slim has two main components: A\n",
    "[Dataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset.py) and a \n",
    "[DatasetDataProvider](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py). The former is a descriptor of a dataset, while the latter performs the actions necessary for actually reading the data. Lets look at each one in detail:\n",
    "\n",
    "\n",
    "## Dataset\n",
    "A TF-Slim\n",
    "[Dataset](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset.py)\n",
    "contains descriptive information about a dataset necessary for reading it, such as the list of data files and how to decode them. It also contains metadata including class labels, the size of the train/test splits and descriptions of the tensors that the dataset provides. For example, some datasets contain images with labels. Others augment this data with bounding box annotations, etc. The Dataset object allows us to write generic code using the same API, regardless of the data content and encoding type.\n",
    "\n",
    "TF-Slim's Dataset works especially well when the data is stored as a (possibly sharded)\n",
    "[TFRecords file](https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/index.html#file-formats), where each record contains a [tf.train.Example protocol buffer](https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/core/example/example.proto).\n",
    "TF-Slim uses a consistent convention for naming the keys and values inside each Example record. \n",
    "\n",
    "## DatasetDataProvider\n",
    "\n",
    "A\n",
    "[DatasetDataProvider](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py) is a class which actually reads the data from a dataset. It is highly configurable to read the data in various ways that may make a big impact on the efficiency of your training process. For example, it can be single or multi-threaded. If your data is sharded across many files, it can read each files serially, or from every file simultaneously.\n",
    "\n",
    "## Demo: The Flowers Dataset\n",
    "\n",
    "For convenience, we've include scripts to convert several common image datasets into TFRecord format and have provided\n",
    "the Dataset descriptor files necessary for reading them. We demonstrate how easy it is to use these dataset via the Flowers dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Flowers Dataset\n",
    "<a id='DownloadFlowers'></a>\n",
    "\n",
    "We've made available a tarball of the Flowers dataset which has already been converted to TFRecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading flowers.tar.gz 12.0%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d50fd86be507>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflowers_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdataset_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_uncompress_tarball\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflowers_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jonathansherman/ml-summer/slim-hub-api/datasets/dataset_utils.py\u001b[0m in \u001b[0;36mdownload_and_uncompress_tarball\u001b[0;34m(tarball_url, dataset_dir)\u001b[0m\n\u001b[1;32m     92\u001b[0m         filename, float(count * block_size) / float(total_size) * 100.0))\n\u001b[1;32m     93\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m   \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarball_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0mstatinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    446\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import dataset_utils\n",
    "\n",
    "url = \"http://download.tensorflow.org/data/flowers.tar.gz\"\n",
    "flowers_data_dir = '/tmp/flowers'\n",
    "\n",
    "if not tf.gfile.Exists(flowers_data_dir):\n",
    "    tf.gfile.MakeDirs(flowers_data_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, flowers_data_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import flowers\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "with tf.Graph().as_default(): \n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32, common_queue_min=1)\n",
    "    image, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    with tf.Session() as sess:    \n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            for i in range(4):\n",
    "                np_image, np_label = sess.run([image, label])\n",
    "                height, width, _ = np_image.shape\n",
    "                class_name = name = dataset.labels_to_names[np_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(np_image)\n",
    "                plt.title('%s, %d x %d' % (name, height, width))\n",
    "                plt.axis('off')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural nets (CNNs).\n",
    "<a id='CNN'></a>\n",
    "\n",
    "In this section, we show how to train an image classifier using a simple CNN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model.\n",
    "\n",
    "Below we define a simple CNN. Note that the output layer is linear function - we will apply softmax transformation externally to the model, either in the loss function (for training), or in the prediction function (during testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_cnn(images, num_classes, is_training):  # is_training is not used...\n",
    "    with slim.arg_scope([slim.max_pool2d], kernel_size=[3, 3], stride=2):\n",
    "        net = slim.conv2d(images, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.conv2d(net, 64, [5, 5])\n",
    "        net = slim.max_pool2d(net)\n",
    "        net = slim.flatten(net)\n",
    "        net = slim.fully_connected(net, 192)\n",
    "        net = slim.fully_connected(net, num_classes, activation_fn=None)       \n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the model to some randomly generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # The model can handle any input size because the first layer is convolutional.\n",
    "    # The size of the model is determined when image_node is first passed into the my_cnn function.\n",
    "    # Once the variables are initialized, the size of all the weight matrices is fixed.\n",
    "    # Because of the fully connected layers, this means that all subsequent images must have the same\n",
    "    # input size as the first image.\n",
    "    batch_size, height, width, channels = 3, 28, 28, 3\n",
    "    images = tf.random_uniform([batch_size, height, width, channels], maxval=1)\n",
    "    \n",
    "    # Create the model.\n",
    "    num_classes = 10\n",
    "    logits = my_cnn(images, num_classes, is_training=True)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Initialize all the variables (including parameters) randomly.\n",
    "    init_op = tf.global_variables_initializer()\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "        # Run the init_op, evaluate the model outputs and print the results:\n",
    "        sess.run(init_op)\n",
    "        probabilities = sess.run(probabilities)\n",
    "        \n",
    "print('Probabilities Shape:')\n",
    "print(probabilities.shape)  # batch_size x num_classes \n",
    "\n",
    "print('\\nProbabilities:')\n",
    "print(probabilities)\n",
    "\n",
    "print('\\nSumming across all classes (Should equal 1):')\n",
    "print(np.sum(probabilities, 1)) # Each row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the Flowers dataset.\n",
    "\n",
    "Before starting, make sure you've run the code to <a href=\"#DownloadFlowers\">Download the Flowers</a> dataset. Now, we'll get a sense of what it looks like to use TF-Slim's training functions found in\n",
    "[learning.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/learning.py). First, we'll create a function, `load_batch`, that loads batches of dataset from a dataset. Next, we'll train a model for a single step (just to demonstrate the API), and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import inception_preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8)\n",
    "    image_raw, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import flowers\n",
    "\n",
    "# This might take a few minutes.\n",
    "train_dir = '/tmp/tfslim_model/'\n",
    "print('Will save model to %s' % train_dir)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    images, _, labels = load_batch(dataset)\n",
    "  \n",
    "    # Create the model:\n",
    "    logits = my_cnn(images, num_classes=dataset.num_classes, is_training=True)\n",
    " \n",
    "    # Specify the loss function:\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    tf.summary.scalar('losses/Total Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "      train_op,\n",
    "      logdir=train_dir,\n",
    "      number_of_steps=1, # For speed, we just do 1 epoch\n",
    "      save_summaries_secs=1)\n",
    "  \n",
    "    print('Finished training. Final batch loss %d' % final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate some metrics.\n",
    "\n",
    "As we discussed above, we can compute various metrics besides the loss.\n",
    "Below we show how to compute prediction accuracy of the trained model, as well as top-5 classification accuracy. (The difference between evaluation and evaluation_loop is that the latter writes the results to a log directory, so they can be viewed in tensorboard.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import flowers\n",
    "\n",
    "# This might take a few minutes.\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "    \n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    images, _, labels = load_batch(dataset)\n",
    "    \n",
    "    logits = my_cnn(images, num_classes=dataset.num_classes, is_training=False)\n",
    "    predictions = tf.argmax(logits, 1)\n",
    "    \n",
    "    # Define the metrics:\n",
    "    names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "        'eval/Accuracy': slim.metrics.streaming_accuracy(predictions, labels),\n",
    "        'eval/Recall@5': slim.metrics.streaming_recall_at_k(logits, labels, 5),\n",
    "    })\n",
    "\n",
    "    print('Running evaluation Loop...')\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    metric_values = slim.evaluation.evaluate_once(\n",
    "        master='',\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        logdir=train_dir,\n",
    "        eval_op=names_to_updates.values(),\n",
    "        final_op=names_to_values.values())\n",
    "\n",
    "    names_to_values = dict(zip(names_to_values.keys(), metric_values))\n",
    "    for name in names_to_values:\n",
    "        print('%s: %f' % (name, names_to_values[name]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using pre-trained models\n",
    "<a id='Pretrained'></a>\n",
    "\n",
    "Neural nets work best when they have many parameters, making them very flexible function approximators.\n",
    "However, this  means they must be trained on big datasets. Since this process is slow, we provide various pre-trained models - see the list [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models).\n",
    "\n",
    "\n",
    "You can either use these models as-is, or you can perform \"surgery\" on them, to modify them for some other task. For example, it is common to \"chop off\" the final pre-softmax layer, and replace it with a new set of weights corresponding to some new set of labels. You can then quickly fine tune the new model on a small new dataset. We illustrate this below, using inception-v1 as the base model. While models like Inception V3 are more powerful, Inception V1 is used for speed purposes.\n",
    "\n",
    "Take into account that VGG and ResNet final layers have only 1000 outputs rather than 1001. The ImageNet dataset provied has an empty background class which can be used to fine-tune the model to other tasks. VGG and ResNet models provided here don't use that class. We provide two examples of using pretrained models: Inception V1 and VGG-19 models to highlight this difference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Inception V1 checkpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import dataset_utils\n",
    "\n",
    "url = \"http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\"\n",
    "checkpoints_dir = '/tmp/checkpoints'\n",
    "\n",
    "if not tf.gfile.Exists(checkpoints_dir):\n",
    "    tf.gfile.MakeDirs(checkpoints_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Apply Pre-trained Inception V1 model to Images.\n",
    "\n",
    "We have to convert each image to the size expected by the model checkpoint.\n",
    "There is no easy way to determine this size from the checkpoint itself.\n",
    "So we use a preprocessor to enforce this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import urllib2 as urllib\n",
    "except ImportError:\n",
    "    import urllib.request as urllib\n",
    "\n",
    "from datasets import imagenet\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    url = 'https://upload.wikimedia.org/wikipedia/commons/7/70/EnglishCockerSpaniel_simon.jpg'\n",
    "    image_string = urllib.urlopen(url).read()\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    processed_image = inception_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\n",
    "    processed_images  = tf.expand_dims(processed_image, 0)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(processed_images, num_classes=1001, is_training=False)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'inception_v1.ckpt'),\n",
    "        slim.get_model_variables('InceptionV1'))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_fn(sess)\n",
    "        np_image, probabilities = sess.run([image, probabilities])\n",
    "        probabilities = probabilities[0, 0:]\n",
    "        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.imshow(np_image.astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    names = imagenet.create_readable_names_for_imagenet_labels()\n",
    "    for i in range(5):\n",
    "        index = sorted_inds[i]\n",
    "        print('Probability %0.2f%% => [%s]' % (probabilities[index] * 100, names[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the VGG-16 checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import dataset_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "url = \"http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\"\n",
    "checkpoints_dir = '/tmp/checkpoints'\n",
    "\n",
    "if not tf.gfile.Exists(checkpoints_dir):\n",
    "    tf.gfile.MakeDirs(checkpoints_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Apply Pre-trained VGG-16 model to Images.\n",
    "\n",
    "We have to convert each image to the size expected by the model checkpoint.\n",
    "There is no easy way to determine this size from the checkpoint itself.\n",
    "So we use a preprocessor to enforce this. Pay attention to the difference caused by 1000 classes instead of 1001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    import urllib2\n",
    "except ImportError:\n",
    "    import urllib.request as urllib\n",
    "\n",
    "from datasets import imagenet\n",
    "from nets import vgg\n",
    "from preprocessing import vgg_preprocessing\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "image_size = vgg.vgg_16.default_image_size\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    url = 'https://upload.wikimedia.org/wikipedia/commons/d/d9/First_Student_IC_school_bus_202076.jpg'\n",
    "    image_string = urllib.urlopen(url).read()\n",
    "    image = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    processed_image = vgg_preprocessing.preprocess_image(image, image_size, image_size, is_training=False)\n",
    "    processed_images  = tf.expand_dims(processed_image, 0)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "        # 1000 classes instead of 1001.\n",
    "        logits, _ = vgg.vgg_16(processed_images, num_classes=1000, is_training=False)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "        os.path.join(checkpoints_dir, 'vgg_16.ckpt'),\n",
    "        slim.get_model_variables('vgg_16'))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init_fn(sess)\n",
    "        np_image, probabilities = sess.run([image, probabilities])\n",
    "        probabilities = probabilities[0, 0:]\n",
    "        sorted_inds = [i[0] for i in sorted(enumerate(-probabilities), key=lambda x:x[1])]\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.imshow(np_image.astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    names = imagenet.create_readable_names_for_imagenet_labels()\n",
    "    for i in range(5):\n",
    "        index = sorted_inds[i]\n",
    "        # Shift the index of a class name by one. \n",
    "        print('Probability %0.2f%% => [%s]' % (probabilities[index] * 100, names[index+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model on a different set of labels.\n",
    "\n",
    "We will fine tune the inception model on the Flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note that this may take several minutes.\n",
    "\n",
    "import os\n",
    "\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "\n",
    "\n",
    "def get_init_fn():\n",
    "    \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "    checkpoint_exclude_scopes=[\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "    \n",
    "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                break\n",
    "        else:\n",
    "            variables_to_restore.append(var)\n",
    "\n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "      os.path.join(checkpoints_dir, 'inception_v1.ckpt'),\n",
    "      variables_to_restore)\n",
    "\n",
    "\n",
    "train_dir = '/tmp/inception_finetuned/'\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    images, _, labels = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "        \n",
    "    # Specify the loss function:\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    tf.summary.scalar('losses/Total Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        init_fn=get_init_fn(),\n",
    "        number_of_steps=2)\n",
    "        \n",
    "  \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply fine tuned model to some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "batch_size = 3\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    images, images_raw, labels = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.initialize_local_variables())\n",
    "            init_fn(sess)\n",
    "            np_probabilities, np_images_raw, np_labels = sess.run([probabilities, images_raw, labels])\n",
    "    \n",
    "            for i in range(batch_size): \n",
    "                image = np_images_raw[i, :, :, :]\n",
    "                true_label = np_labels[i]\n",
    "                predicted_label = np.argmax(np_probabilities[i, :])\n",
    "                predicted_name = dataset.labels_to_names[predicted_label]\n",
    "                true_name = dataset.labels_to_names[true_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(image.astype(np.uint8))\n",
    "                plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
    "                plt.axis('off')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
