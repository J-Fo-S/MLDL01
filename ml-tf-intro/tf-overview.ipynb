{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've gotten our hands dirty with Python and some of its important operations, it's time to take on Tensorflow. Tensorflow is Google's language platform for deep learning, and it is one - if not the - most important platforms for deep learning available to the public. It can be used on its own or through higher-level frameworks such as Caffe and Keras, which can be set to use Tensorflow as a backend. \n",
    "\n",
    "*Note: consider adding for visual clarity https://blog.jakuba.net/2017/05/30/tensorflow-visualization.html*\n",
    "\n",
    "### Foreword - Take a Deep Breath..\n",
    "\n",
    "Fear not, but Tensorflow is extraordinarily complex and can be quite complicated - almost too so. It has layers upon layers of abstractions and a horde of APIs which seem to gain and fall from prominence each day. Backwards compatibility is often an issue. Such is the life of a rapidly developin ecosystem I suppose. \n",
    "\n",
    "Because of the above, we will come across very many methods of writing Tensorflow programs as we explore some of the most interesting models currently available. It is COMPLETELY NORMAL to feel overwhelmed and confused at first. Do not worry about this. Our method in this course is not to try and understand everything at once, but focus on areas of interest to understand how they work and how we may work with them. We will expand our knowledge in this fashion, by combining what we learn. In time, the whole picture of Tensorflow and machine/deep learning will become clearer.\n",
    "\n",
    "### High-Level / Low-Level\n",
    "\n",
    "One can create the same model in 30 lines of code or in hundreds of lines. We are going to do both. Why? Well, the 30-line model is so *high-level* that we won't really understand what those 30 lines of code are doing, for everything is hidden in high-level abstractions. So we can look at lower-level code, which will have many more lines but show us more clearly what exactly is happening. Once we understand how the model works on a lower-level, we can work more knowledgeably and quickly with higher-level programming again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow and Python - BFFs Forever?\n",
    "\n",
    "Tensorflow works hand-in-hand with the Python framework and many of its methods are basically exact replicas of Python/Numpy code. This is generally a good thing because it gives us familiarity as we work between the two languages. The reason for this partnership is that their similarities and compatibility support a fundamental difference in execution: **immediate/eager evaluation** vs. **graph computation**\n",
    "\n",
    "In short, Python will evaluate (run) the code as soon as it sees it. Tensorflow, on the other hand, does not immediately run its operations. Instead, it builds a so-called *graph*, which can be considered like a system of how each variable and operation are connected to one another. In order to execute (run) the graph, we must explicitly tell Tensorflow to do so. This will become apparent in the example code below.\n",
    "\n",
    "Final note - this introductory code has been taken from several sources, who often do a better job explaining then I can, so we will follow along with some external sites as we go. To start with, please open [this tutorial](https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/#understanding-tensorflow) to read along with the example code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathansherman/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf graph element: <tf.Variable 'Variable:0' shape=() dtype=int32_ref> python variable execution: 2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# create a tf.Variable\n",
    "one_node = tf.Variable(2)\n",
    "\n",
    "#create a Python variable\n",
    "py_var = 2\n",
    "\n",
    "print(\"tf graph element: %s\" % (one_node,),\"python variable execution: %s\" % (py_var,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Tensorflow code above did not evaluate, but returned instead a graph element called a **tensor** (more on that later). On the other hand, the Python variable was immediately evaluated and returned 2.\n",
    "\n",
    "This creates some very important differences. For example, in Python if I assign 2 to the same variable, we still have 1 variable with a value of 2. If I assign this variable a value of 3, then we have the same 1 variable but with a value of 3. In Tensorflow, the graph does something quite different. Let's see for ourselves below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "print(two_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_4:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "two_node = tf.constant(2)\n",
    "two_node = tf.constant(2)\n",
    "tf.constant(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the first code block we have 'Const' and then in the second code block we have 'Const_4'. This means each time we call tf.constant it adds a tf.constant to the graph regardless of where we assign the variable in our code.\n",
    "\n",
    "If we don't wish for this to happen, then we should create \"pointer\" variables instead of calling new instances of tf variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_3:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "pointer_at_two_node = two_node\n",
    "print(pointer_at_two_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we can notice 2 things. First, we didn't create another tf.constant but simply pointed the variable already storing it. Also, the tf.constant is 'Const_3' instead of 'Const_4' because we created 'Const_4' without assigning it to a Python variable. The variable we are \"pointing\" to contains 'Const_3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we have created so far are called **nodes** in tensorflow - that is, individual values. When we want to compute them together somehow, this is called an **operation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "three_node = tf.constant(3)\n",
    "sum_node = two_node + three_node ## equivalent to tf.add(two_node, three_node)\n",
    "print(sum_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though very simple, we have our 1st **computational graph**. Notice that it has not been evaluated yet, for when we print sum_node we get the operation \"add\", not 5 like we would if this were just in Python.\n",
    "\n",
    "In order to evaluate a graph, we need to begin what is called a **session**. This means the graph is interactive and can be executed (evaluated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(sum_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we want the values of nodes as well as operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run([two_node, sum_node]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can return them to Python variables if we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 5 7\n"
     ]
    }
   ],
   "source": [
    "node_val, op_result = sess.run([two_node, sum_node])\n",
    "print(node_val, op_result, node_val+op_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes when creating a graph, we want to add in values at a later point (for instance, perhaps we need to insert an image file into our graph and evaluate it). To do so, we can make use of **tf.placeholder** and **feed_dict**. As the name suggests, placeholder simply holds an empty location in the graph. It has no value until we pass in a value using feed_dict. \n",
    "\n",
    "Below we'll feed a tf.placeholder with a Python variable of value 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "input_placeholder = tf.placeholder(tf.int32)\n",
    "feed_value = 2\n",
    "print(sess.run(input_placeholder, feed_dict={input_placeholder: feed_value}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've evaluated graphs that contain static or \"no-ancestor\" nodes - they don't update. However, with machine/deep learning, **we often need nodes that update** - this is how they learn! Most of the **parameters** we want to train/learn will be implemented as Tensorflow **variables**. \n",
    "\n",
    "Also, we've only been dealing with **singular values**, called **scalars** so far. Generally, we will be working with **matrices (and \"tensors\")** of varying dimensions. To do this, we assign variables a **shape**, which is their dimensions. \n",
    "\n",
    "Finally, we can also give the variable a **name** to make it easier to identify, or Tensorflow will do it for us (e.g. 'const_5', etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'value' of 'Assign' Op has type int32 that does not match type float32 of argument 'ref'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    511\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    948\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype float32 for Tensor with dtype int32: 'Tensor(\"Const_7:0\", shape=(2, 3), dtype=int32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-5dd14a5ec98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mzero_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0massign_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m    281\u001b[0m     return gen_state_ops.assign(\n\u001b[1;32m    282\u001b[0m         \u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         validate_shape=validate_shape)\n\u001b[0m\u001b[1;32m    284\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m     59\u001b[0m         \u001b[0;34m\"Assign\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    544\u001b[0m                   \u001b[0;34m\"%s type %s of argument '%s'.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m                   (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,\n\u001b[0;32m--> 546\u001b[0;31m                    inferred_from[input_arg.type_attr]))\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m           \u001b[0mtypes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'value' of 'Assign' Op has type int32 that does not match type float32 of argument 'ref'."
     ]
    }
   ],
   "source": [
    "count_variable = tf.get_variable(\"count\", [2, 3])\n",
    "zero_node = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "assign_node = tf.assign(count_variable, zero_node)\n",
    "sess = tf.Session()\n",
    "sess.run(assign_node)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note we received an error exception for data type incompatibility. We often need to declare which data type we want to work with to avoid this error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable count already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-10-5dd14a5ec98a>\", line 1, in <module>\n    count_variable = tf.get_variable(\"count\", [2, 3])\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1004630d0987>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcount_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mzero_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0massign_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable count already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-10-5dd14a5ec98a>\", line 1, in <module>\n    count_variable = tf.get_variable(\"count\", [2, 3])\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "count_variable = tf.get_variable(\"count\", [2, 3], dtype=tf.float32)\n",
    "zero_node = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "assign_node = tf.assign(count_variable, zero_node)\n",
    "sess = tf.Session()\n",
    "sess.run(assign_node)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, now another error exception - this time for variable reuse. This is because we have given it a specific name, so Tensorflow cannot simply add another variable like it did when we allowed it to autoname variables. And because the error in our code was further below the count_variable, it thinks we are trying to add it again and it is blocking us. Simple workaround when this happens - comment out the count_variable until the block of code works, then include it next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "#count_variable = tf.get_variable(\"count\", [2, 3], dtype=tf.float32)\n",
    "zero_node = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "assign_node = tf.assign(count_variable, zero_node)\n",
    "sess = tf.Session()\n",
    "sess.run(assign_node)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or reset the \"default graph\" which will clear variables and let us start anew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "count_variable = tf.get_variable(\"count\", [2, 3], dtype=tf.float32)\n",
    "zero_node = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.float32)\n",
    "assign_node = tf.assign(count_variable, zero_node)\n",
    "sess = tf.Session()\n",
    "sess.run(assign_node)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so it finally worked, but perhaps is still confusing. Why this \"get_variable\" and \"assign\" process? Why can't we just create a variable in one step. Well, we can with **tf.Variable()**, but the \"pointer\" method above makes sure we aren't creating unnecessary graph elements like we've shown before. For instance, if there already is a variable with the name we are looking for, we don't want to create another. tf.Variable() will always try to create another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable count already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-13-63a0e12acad5>\", line 2, in <module>\n    count_variable = tf.get_variable(\"count\", [2, 3], dtype=tf.float32)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-d3fe2832e913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mblocked_additional_count_variable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1315\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1318\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1319\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1077\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    731\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 733\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    734\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable count already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-13-63a0e12acad5>\", line 2, in <module>\n    count_variable = tf.get_variable(\"count\", [2, 3], dtype=tf.float32)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "blocked_additional_count_variable = tf.get_variable(\"count\", [2, 3], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'count_1:0' shape=() dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "create_additional_count_var = tf.Variable(0, name = 'count')\n",
    "print(create_additional_count_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above, get_variable was an error exception as we expected, whereas tf.Variable created a new node with a 0 value, autonamed 'count_1'. If this is not what we wanted, we will have problems with our graph.\n",
    "\n",
    "Finally, there is another way to create and evaluate variables other than using the 'assign' method. We can use \"initializers\", which will set all variables or constants to an initial value. Since these will generally be updated by our training, we often don't care what the initial value is, or just set it to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "const_init_node = tf.constant_initializer(0.)\n",
    "count_variable = tf.get_variable(\"count\", [], initializer=const_init_node)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh yeah, that annoying error again. We've \"accidentally\" created a bunch of nodes we don't want, and are trying to change an existing node. Let's reset all of the variables and get this right by using **tf.reset_default_graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "const_init_node = tf.constant_initializer(0.)\n",
    "count_variable = tf.get_variable(\"count\", [], initializer=const_init_node)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully this gives you an idea of how Tensorflow works with but is different than Python, and how the concept of building a graph with nodes and operations differes and *then* evaluating it is different than the immediate evaluation we are probably used to with Python or other coding languages.\n",
    "\n",
    "There is plenty more to learn about Tensorflow, but this is a good start. \n",
    "\n",
    "Let's see how this is put together in a toy **linear regression** example.\n",
    "\n",
    "Most importantly, pay attention to how the variables are updated as the model trains and evaluates. This is called **optimization**, and a typical optimization will do something along the likes of the following:\n",
    "\n",
    "1. Get an input and true_output\n",
    "2. Compute a “guess” based on the input and your parameters\n",
    "3. Compute a “loss” based on the difference between your guess and the true_output\n",
    "4. Update the parameters according to the gradient of the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build the graph\n",
    "## first set up the parameters\n",
    "m = tf.get_variable(\"m\", [], initializer=tf.constant_initializer(0.))\n",
    "b = tf.get_variable(\"b\", [], initializer=tf.constant_initializer(0.))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "## then set up the computations\n",
    "input_placeholder = tf.placeholder(tf.float32)\n",
    "output_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "x = input_placeholder\n",
    "y = output_placeholder\n",
    "y_guess = m * x + b\n",
    "\n",
    "loss = tf.square(y - y_guess)\n",
    "\n",
    "## A \"Hyperparameter\" - you control this!\n",
    "learning_rate = 0.001\n",
    "\n",
    "## finally, set up the optimizer and minimization node\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "### start the session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "### perform the training loop\n",
    "import random\n",
    "\n",
    "## set up problem\n",
    "true_m = random.random()\n",
    "true_b = random.random()\n",
    "\n",
    "for update_i in range(10000):\n",
    "    ## (1) get the input and output\n",
    "    input_data = random.random()\n",
    "    output_data = true_m * input_data + true_b\n",
    "    \n",
    "    ## (2), (3), and (4) all take place within a single call to sess.run()!\n",
    "    _loss, _ = sess.run([loss, train_op], feed_dict={input_placeholder: input_data, output_placeholder: output_data})\n",
    "    print(update_i, _loss)\n",
    "    \n",
    "\n",
    "### finally, print out the values we learned for our two variables\n",
    "print(\"True parameters:     m=%.4f, b=%.4f\" % (true_m, true_b))\n",
    "print(\"Learned parameters:  m=%.4f, b=%.4f\" % tuple(sess.run([m, b])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often times if we are developing a model, we want to inspect the graph and its variables and performance. We can (and usually will) just print or plot the values for quick visualization. However, Tensorflow has greater tools for inspecting our graph and model, such as Tensorboard. Below we will run the same toy linear regression model but save the variable data we are interested in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "### build the graph\n",
    "## first set up the parameters\n",
    "m = tf.get_variable(\"m\", [], initializer=tf.constant_initializer(0.))\n",
    "## save our variable for Tensorboard\n",
    "tf.summary.histogram(\"m_prediction\",m)\n",
    "b = tf.get_variable(\"b\", [], initializer=tf.constant_initializer(0.))\n",
    "tf.summary.histogram(\"b_prediction\",b)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "## then set up the computations\n",
    "input_placeholder = tf.placeholder(tf.float32, name=\"input\")\n",
    "output_placeholder = tf.placeholder(tf.float32, name=\"output\")\n",
    "\n",
    "x = input_placeholder\n",
    "y = output_placeholder\n",
    "tf.summary.histogram(\"y_true\",output_placeholder)\n",
    "y_guess = m * x + b\n",
    "tf.summary.histogram(\"y_guess\",y_guess)\n",
    "\n",
    "loss = tf.square(y - y_guess)\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "## finally, set up the optimizer and minimization node\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "### start the session\n",
    "sess = tf.Session()\n",
    "## merge all our Tensorboard summaries and create file directory\n",
    "summaryMerged = tf.summary.merge_all()\n",
    "filename=\"./summary_log/run\"+datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "writer = tf.summary.FileWriter(filename, sess.graph)\n",
    "\n",
    "## run variables initializer\n",
    "sess.run(init)\n",
    "\n",
    "### perform the training loop\n",
    "import random\n",
    "\n",
    "## set up problem\n",
    "true_m = random.random()\n",
    "true_b = random.random()\n",
    "\n",
    "for update_i in range(20000):\n",
    "    ## (1) get the input and output\n",
    "    input_data = random.random()\n",
    "    output_data = true_m * input_data + true_b\n",
    "    \n",
    "    ## (2), (3), and (4) all take place within a single call to sess.run()!\n",
    "    _loss, _, sumOut = sess.run([loss, train_op,summaryMerged], feed_dict={input_placeholder: input_data, output_placeholder: output_data})\n",
    "    print(update_i, _loss)\n",
    "    \n",
    "    if update_i % 100 == 0:\n",
    "        ## write all of the variables to file\n",
    "        writer.add_summary(sumOut, update_i)\n",
    "\n",
    "### finally, print out the values we learned for our two variables\n",
    "print(\"True parameters:     m=%.4f, b=%.4f\" % (true_m, true_b))\n",
    "print(\"Learned parameters:  m=%.4f, b=%.4f\" % tuple(sess.run([m, b])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open up your terminal console and cd to the same file directory location our program is in\n",
    "\n",
    "Then run ```tensorboard --logdir=./summary_log/``` and copy the location http address into a new browser window. You should see our graph, scalars, histogram and even more. Run the model more than once, and you can compare the results between runs. Very useful!\n",
    "\n",
    "For more details see the source I adapted this from: https://thecodacus.com/tensorboard-tutorial-visualize-networks-graphically/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also just visualize our graph inline with Jupyter via [the following code](https://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph().as_graph_def())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can zoom and click each node and operation to see its inputs and outputs, which can be very helpful for debugging or just understanding what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore and importantly, we can save and reload trained models. This is essential for any serious machine/deep learning project that takes significant time to train. Now, we can train something for weeks or even months, or even forever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "### build the graph\n",
    "## first set up the parameters\n",
    "m = tf.get_variable(\"m\", [], initializer=tf.constant_initializer(0.))\n",
    "## save our variable for Tensorboard\n",
    "tf.summary.histogram(\"m_prediction\",m)\n",
    "b = tf.get_variable(\"b\", [], initializer=tf.constant_initializer(0.))\n",
    "tf.summary.histogram(\"b_prediction\",b)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "## then set up the computations\n",
    "input_placeholder = tf.placeholder(tf.float32)\n",
    "output_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "x = input_placeholder\n",
    "y = output_placeholder\n",
    "tf.summary.histogram(\"y_true\",output_placeholder)\n",
    "y_guess = m * x + b\n",
    "tf.summary.histogram(\"y_guess\",y_guess)\n",
    "\n",
    "loss = tf.square(y - y_guess)\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "# a \"hyperparameter\"\n",
    "learning_rate = 0.001\n",
    "\n",
    "## finally, set up the optimizer and minimization node\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "### start the session\n",
    "sess = tf.Session()\n",
    "## merge all our Tensorboard summaries and create file directory\n",
    "summaryMerged = tf.summary.merge_all()\n",
    "filename=\"./summary_log/run\"+datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%s\")\n",
    "writer = tf.summary.FileWriter(filename, sess.graph)\n",
    "\n",
    "## create tf.saver\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "## run variables initializer\n",
    "sess.run(init)\n",
    "\n",
    "### perform the training loop\n",
    "import random\n",
    "\n",
    "## set up problem\n",
    "true_m = random.random()\n",
    "true_b = random.random()\n",
    "\n",
    "for update_i in range(20000):\n",
    "    ## (1) get the input and output\n",
    "    input_data = random.random()\n",
    "    output_data = true_m * input_data + true_b\n",
    "    \n",
    "    ## (2), (3), and (4) all take place within a single call to sess.run()!\n",
    "    _loss, _, sumOut = sess.run([loss, train_op,summaryMerged], feed_dict={input_placeholder: input_data, output_placeholder: output_data})\n",
    "    print(update_i, _loss)\n",
    "    \n",
    "    if update_i % 100 == 0:\n",
    "        ## write all of the variables to file\n",
    "        writer.add_summary(sumOut, update_i)\n",
    "\n",
    "## save to directory\n",
    "save_path = saver.save(sess, './model/weights')\n",
    "\n",
    "### finally, print out the values we learned for our two variables\n",
    "print(\"True parameters:     m=%.4f, b=%.4f\" % (true_m, true_b))\n",
    "print(\"Learned parameters:  m=%.4f, b=%.4f\" % tuple(sess.run([m, b])))\n",
    "print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we should have a folder \"model\" with our weight files saved (these are all of our variables). We can now reload the model and continue working with it, without having to retrain from scratch. More on working with and saving models here: https://www.tensorflow.org/guide/saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Reset all variables to initial state\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Verify reset variable\n",
    "    print('reset value of var m',m.eval())\n",
    "    \n",
    "    reader = tf.train.Saver()\n",
    "    reader.restore(sess, './model/weights')\n",
    "    \n",
    "    # Verify restored variable\n",
    "    print('restored value of var m',m.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One tricky but essential aspect of working with Tensorflow is managing graphs and sessions. Recall that the graph contains all the nodes and operations we build for our learning model. We call a session when we want to evaluate the graph.\n",
    "\n",
    "Things can get confusing though if we:\n",
    "1. have more than one graph, \n",
    "2. need to run more than one session,\n",
    "3. want to reset or update graph elements\n",
    "4. want to access graph elements for input or output\n",
    "\n",
    "We won't cover all techniques here, but let's get a bit more familiar with graph and session usage before moving on.\n",
    "\n",
    "We can assign our default graph (which is automatically instantiated by Tensorflow) to a variable if we wish (though normally we wouldn't - this is just to demonstrate. Normally, you can name a graph if you want at the outset or when constructing a 2nd, different graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify it's the graph we want\n",
    "show_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to access all of the nodes in our graph is to call them via graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphdef = graph.as_graph_def()\n",
    "for node in graphdef.node:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do similarly access all of the operations in our graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in graph.get_operations(): \n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get a specific operation by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "some_op = graph.get_operation_by_name(\"gradients/mul_grad/Shape\")\n",
    "print(some_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or layer and type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [op.name for op in graph.get_operations() if op.type=='Const' and 'gradients' in op.name]\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#close and clear our default graph\n",
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "eval_graph = tf.Graph()\n",
    "infer_graph = tf.Graph()\n",
    "\n",
    "x = tf.constant(0)\n",
    "\n",
    "with train_graph.as_default():\n",
    "    with tf.variable_scope(\"share\", reuse=True):\n",
    "        a = tf.constant([[-1, -2, -23], [-4, -5, -6]])\n",
    "        b = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
    "        c = tf.add(a,b)\n",
    "\n",
    "with eval_graph.as_default():\n",
    "    with tf.variable_scope(\"share_again\", reuse=True):\n",
    "        c1 = c\n",
    "\n",
    "with infer_graph.as_default():\n",
    "    with tf.variable_scope(\"sharing_through_2\", reuse=True):\n",
    "        c2 = c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graph(train_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
