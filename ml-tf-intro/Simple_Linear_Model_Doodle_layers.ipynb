{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial #01\n",
    "# Simple Linear Model with Google Doodle\n",
    "\n",
    "by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
    "/ [GitHub](https://github.com/Hvass-Labs/TensorFlow-Tutorials) / [Videos on YouTube](https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial demonstrates the basic workflow of using TensorFlow with a simple linear model. Instead of the ubiquitous MNIST, we will utilize the Google Doodles dataset and define and optimize a simple mathematical model in TensorFlow. The results are then plotted and discussed.\n",
    "\n",
    "Start here: https://quickdraw.withgoogle.com/data\n",
    "\n",
    "See also: https://www.tensorflow.org/versions/r1.0/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonathansherman/anaconda3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was developed using Python 3.6.1 (Anaconda) and TensorFlow version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#data = input_data.read_data_sets(\"data/MNIST/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of MNIST data, we will load our doodle sets, which are in a different file format and must go through some formatting before we can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name-of-dataset']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "xtrn = h5py.File('data/doodle_data/x_train.h5', 'r')\n",
    "list(xtrn)\n",
    "ytrn = h5py.File('data/doodle_data/y_train.h5', 'r')\n",
    "list(ytrn)\n",
    "xtst = h5py.File('data/doodle_data/x_test.h5', 'r')\n",
    "list(xtst)\n",
    "ytst = h5py.File('data/doodle_data/y_test.h5', 'r')\n",
    "list(ytst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"name-of-dataset\": shape (16000,), type \"<i8\">"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrn.get('name-of-dataset')\n",
    "ytrn.get('name-of-dataset')\n",
    "xtst.get('name-of-dataset')\n",
    "ytst.get('name-of-dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrn_data = list(xtrn['name-of-dataset'])\n",
    "ytrn_data = list(ytrn['name-of-dataset'])\n",
    "xtst_data = list(xtst['name-of-dataset'])\n",
    "ytst_data = list(ytst['name-of-dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrn_data = np.array(xtrn_data)\n",
    "ytrn_data = np.array(ytrn_data)\n",
    "xtst_data = np.array(xtst_data)\n",
    "ytst_data = np.array(ytst_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of 80000 images with classification labels (numerical). They are split into 2 sets, training and testing.\n",
    "\n",
    "This means we are following the more basic approach (2 set) as described here: https://developers.google.com/machine-learning/crash-course/training-and-test-sets/splitting-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set:\t\t64000\n",
      "- Test-set:\t\t16000\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of:\")\n",
    "print(\"- Training-set:\\t\\t{}\".format(len(ytrn_data)))\n",
    "print(\"- Test-set:\\t\\t{}\".format(len(ytst_data)))\n",
    "#print(\"- Validation-set:\\t{}\".format(len(data.validation.labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.27450982, 0.3647059 ,\n",
       "       0.33333334, 0.29411766, 0.05882353, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.01960784, 0.9882353 , 1.        , 1.        , 1.        ,\n",
       "       0.972549  , 0.6666667 , 0.11764706, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.09019608, 1.        ,\n",
       "       0.46666667, 0.13333334, 0.22352941, 0.5254902 , 0.9372549 ,\n",
       "       0.9490196 , 0.42352942, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.15686275, 1.        , 0.31764707, 0.        ,\n",
       "       0.        , 0.        , 0.09803922, 0.6784314 , 1.        ,\n",
       "       0.7294118 , 0.03137255, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.22745098,\n",
       "       1.        , 0.24705882, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.36078432, 0.9882353 , 0.6745098 ,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.3529412 , 1.        , 0.15294118,\n",
       "       0.34509805, 0.63529414, 0.1254902 , 0.        , 0.        ,\n",
       "       0.        , 0.40392157, 1.        , 0.43529412, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.60784316, 0.9254902 , 0.00392157, 0.6313726 , 1.        ,\n",
       "       0.9647059 , 0.49803922, 0.03529412, 0.        , 0.        ,\n",
       "       0.7137255 , 0.89411765, 0.02745098, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.11372549, 0.96862745, 0.5803922 ,\n",
       "       0.        , 0.6745098 , 0.84705883, 0.61960787, 0.99215686,\n",
       "       0.8745098 , 0.1764706 , 0.        , 0.23137255, 1.        ,\n",
       "       0.40392157, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6117647 , 0.9607843 , 0.09803922, 0.        , 0.7137255 ,\n",
       "       0.75686276, 0.        , 0.21568628, 0.70980394, 0.24313726,\n",
       "       0.        , 0.        , 0.8       , 0.7254902 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.25882354, 0.9843137 , 0.5254902 ,\n",
       "       0.        , 0.        , 0.7529412 , 0.7176471 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.6313726 , 0.8509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.29411766,\n",
       "       0.96862745, 0.79607844, 0.05098039, 0.        , 0.        ,\n",
       "       0.79607844, 0.6745098 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.5058824 , 0.972549  ,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.01176471, 0.9098039 , 0.78039217, 0.05490196,\n",
       "       0.        , 0.        , 0.        , 0.8352941 , 0.63529414,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.7137255 , 0.9254902 , 0.01176471, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03529412,\n",
       "       1.        , 0.43529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.8901961 , 0.59607846, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.3137255 , 1.        ,\n",
       "       0.40784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00392157, 0.99215686, 0.47843137,\n",
       "       0.        , 0.        , 0.        , 0.21960784, 1.        ,\n",
       "       0.3764706 , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.23529412, 0.92941177, 0.79607844, 0.00784314, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.9490196 , 0.52156866, 0.        , 0.        ,\n",
       "       0.        , 0.6392157 , 0.93333334, 0.03921569, 0.        ,\n",
       "       0.        , 0.        , 0.3254902 , 0.96862745, 0.8235294 ,\n",
       "       0.10588235, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.9098039 ,\n",
       "       0.5647059 , 0.        , 0.        , 0.03137255, 0.972549  ,\n",
       "       0.9764706 , 0.84313726, 0.72156864, 0.15294118, 0.43137255,\n",
       "       0.99215686, 0.74509805, 0.05882353, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.74509805, 0.46666667, 0.        ,\n",
       "       0.        , 0.        , 0.3764706 , 0.54509807, 0.63529414,\n",
       "       0.8980392 , 0.92941177, 1.        , 0.6431373 , 0.02352941,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.6117647 , 0.96862745,\n",
       "       0.78039217, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.8       , 0.8980392 , 0.9137255 , 0.05098039,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 1.        ,\n",
       "       0.50980395, 0.9764706 , 0.63529414, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08627451, 0.05490196,\n",
       "       0.        , 0.6784314 , 0.9019608 , 0.02745098, 0.45882353,\n",
       "       1.        , 0.36078432, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.7411765 , 0.87058824, 0.17254902, 0.5568628 ,\n",
       "       0.36078432, 0.        , 0.00784314, 0.7294118 , 0.94509804,\n",
       "       0.12156863, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.17254902,\n",
       "       0.8745098 , 0.9490196 , 0.34901962, 0.00392157, 0.        ,\n",
       "       0.        , 0.11372549, 1.        , 0.42745098, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.08235294, 0.75686276,\n",
       "       1.        , 0.8156863 , 0.31764707, 0.05098039, 0.14117648,\n",
       "       0.9843137 , 0.5294118 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.32941177, 0.8235294 ,\n",
       "       1.        , 1.        , 1.        , 0.92156863, 0.24705882,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.00784314, 0.2901961 , 0.42352942,\n",
       "       0.4392157 , 0.08235294, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtst_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, our data has integers assigned for its label, but is not in the one-hot vector form. We will need to convert the vector into a matrix of one-hot vectors. Turns out you can do it in one line of code - not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrn_onehot = np.eye(np.max(ytrn_data)+1)[ytrn_data]\n",
    "\n",
    "ytst_onehot = np.eye(np.max(ytst_data)+1)[ytst_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data-set has been now been converted to one-hot encoding. This means the labels have been converted from a single number to a vector whose length equals the number of possible classes. All elements of the vector are zero except for the $i$'th element which is one and means the class is $i$. For example, the One-Hot encoded labels for the first 5 images in the test-set are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytst_onehot[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the classes as single numbers for various comparisons and performance measures, so we convert the One-Hot encoded vectors to a single number by taking the index of the highest element."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the class for the first five images in the test-set. Compare these to the One-Hot encoded vectors above. For example, the class for the first image is at index 4, which corresponds to a One-Hot encoded vector where all elements are zero except for the element at index 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, 1, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytst_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dimensions are used in several places in the source-code below. In computer programming it is generally best to use variables and constants rather than having to hard-code specific numbers every time that number is used. This means the numbers only have to be changed in one single place. Ideally these would be inferred from the data that has been read, but here we just write the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We know that MNIST images are 28 pixels in each dimension.\n",
    "img_size = 28\n",
    "\n",
    "# Images are stored in one-dimensional arrays of this length (e.g. 28 x 28 = 784).\n",
    "img_size_flat = img_size * img_size\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size, img_size)\n",
    "\n",
    "# Number of classes, one class for each of 10 digits.\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function for plotting images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to plot 9 images in a 3x3 grid, and writing the true and predicted classes below each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot a few images to see if data is correct\n",
    "\n",
    "Check not just the first 9 to see how data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 784) (16000,)\n"
     ]
    }
   ],
   "source": [
    "print(xtst_data.shape, ytst_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUMAAAD5CAYAAAC9FVegAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1JwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 0  full%2Fnumpy_bitmap%2Fbeard.npy\n",
      "file: 1  full%2Fnumpy_bitmap%2Fbinoculars.npy\n",
      "file: 2  full%2Fnumpy_bitmap%2Fbrain.npy\n",
      "file: 3  full%2Fnumpy_bitmap%2Fclarinet.npy\n",
      "file: 4  full%2Fnumpy_bitmap%2Fear.npy\n",
      "file: 5  full%2Fnumpy_bitmap%2Feye.npy\n",
      "file: 6  full%2Fnumpy_bitmap%2Fgoatee.npy\n",
      "file: 7  full%2Fnumpy_bitmap%2Fhelmet.npy\n",
      "file: 8  full%2Fnumpy_bitmap%2Fkeyboard.npy\n",
      "file: 9  full%2Fnumpy_bitmap%2Flollipop.npy\n"
     ]
    }
   ],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = xtst_data[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = ytst_data[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images, cls_true)\n",
    "for indx, file in enumerate(os.listdir('data/doodle_src/')):\n",
    "    print(\"file: {0} \".format(indx), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph\n",
    "\n",
    "The entire purpose of TensorFlow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python. TensorFlow can be more efficient than NumPy because TensorFlow knows the entire computation graph that must be executed, while NumPy only knows the computation of a single mathematical operation at a time.\n",
    "\n",
    "TensorFlow can also automatically calculate the gradients that are needed to optimize the variables of the graph so as to make the model perform better. This is because the graph is a combination of simple mathematical expressions so the gradient of the entire graph can be calculated using the chain-rule for derivatives.\n",
    "\n",
    "A TensorFlow graph consists of the following parts which will be detailed below:\n",
    "\n",
    "* Placeholder variables used to change the input to the graph.\n",
    "* Model variables (called tensors) that are going to be optimized so as to make the model perform better. These are connected as inputs and outputs of \"nodes\"\n",
    "* The nodes of the model which are essentially mathematical functions that calculates some output given the input in the placeholder variables and the model/tensor variables.\n",
    "* A cost measure that can be used to guide the optimization of the variables.\n",
    "* An optimization method which updates the variables of the model.\n",
    "\n",
    "In addition, the TensorFlow graph may also contain various debugging statements e.g. for logging data to be displayed using TensorBoard, which is not covered in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder variables serve as the input to the graph that we may change each time we execute the graph. We call this feeding the placeholder variables and it is demonstrated further below.\n",
    "\n",
    "First we define the placeholder variable for the input images. This allows us to change the images that are input to the TensorFlow graph. This is a so-called tensor, which just means that it is a multi-dimensional vector or matrix. The data-type is set to `float32` and the shape is set to `[None, img_size_flat]`, where `None` means that the tensor may hold an arbitrary number of images with each image being a vector of length `img_size_flat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 784)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, img_size_flat])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the placeholder variable for the true labels associated with the images that were input in the placeholder variable `x`. The shape of this placeholder variable is `[None, num_classes]` which means it may hold an arbitrary number of labels and each label is a vector of length `num_classes` which is 10 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "y_true = tf.placeholder(tf.float32, [None, num_classes])\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have the placeholder variable for the true class of each image in the placeholder variable `x`. These are integers and the dimensionality of this placeholder variable is set to `[None]` which means the placeholder variable is a one-dimensional vector of arbitrary length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n"
     ]
    }
   ],
   "source": [
    "y_true_cls = tf.placeholder(tf.int64, [None])\n",
    "print(y_true_cls.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the placeholder variables that were defined above and which serve as feeding input data into the model, there are also some model variables that must be changed by TensorFlow so as to make the model perform better on the training data.\n",
    "\n",
    "The first variable that must be optimized is called `weights` and is defined here as a TensorFlow variable that must be initialized with zeros and whose shape is `[img_size_flat, num_classes]`, so it is a 2-dimensional tensor (or matrix) with `img_size_flat` rows and `num_classes` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(X, n_input, n_output, activation=None, scope=None):\n",
    "    with tf.variable_scope(scope or \"linear\"):\n",
    "        weights = tf.Variable(tf.zeros([n_input, n_output]))\n",
    "            #initializer=tf.random_normal_initializer(mean=0.0, stddev=0.1))\n",
    "        tf.summary.histogram(\"weights\", weights)\n",
    "        biases = tf.Variable(tf.zeros([n_output]))\n",
    "            #initializer=tf.constant_initializer())\n",
    "        tf.summary.histogram(\"biases\", biases)\n",
    "        if activation is None:\n",
    "            h = tf.matmul(X, weights) + biases\n",
    "        if activation is \"tanh\":\n",
    "            h = tf.nn.tanh(tf.matmul(X, weights) + biases, name='tanh')\n",
    "        if activation is \"sigmoid\":\n",
    "            h = tf.nn.sigmoid(tf.matmul(X, weights) + biases, name='sigmoid')\n",
    "        if activation is \"relu\":\n",
    "            h = tf.nn.relu(tf.matmul(X, weights) + biases, name='relu')\n",
    "        print(tf.shape(h))\n",
    "        return weights, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weights = tf.Variable(tf.zeros([img_size_flat, num_classes]))\n",
    "#tf.summary.histogram(\"weights\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second variable that must be optimized is called `biases` and is defined as a 1-dimensional tensor (or vector) of length `num_classes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#biases = tf.Variable(tf.zeros([num_classes]))\n",
    "#tf.summary.histogram(\"biases\", biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple mathematical model multiplies the images in the placeholder variable `x` with the `weights` and then adds the `biases`.\n",
    "\n",
    "The result is a matrix of shape `[num_images, num_classes]` because `x` has shape `[num_images, img_size_flat]` and `weights` has shape `[img_size_flat, num_classes]`, so the multiplication of those two matrices is a matrix with shape `[num_images, num_classes]` and then the `biases` vector is added to each row of that matrix.\n",
    "\n",
    "Note that the name `logits` is typical TensorFlow terminology, but other people may call the variable something else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"layer_1/Shape:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"layer_2/Shape:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"layer_3/Shape:0\", shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#logits = tf.matmul(x, weights) + biases\n",
    "_, logits_a = linear(x, 784, 256, activation=None, scope='layer_1')\n",
    "_, logits_b = linear(logits_a, 256, 784, activation=None, scope='layer_2')\n",
    "weights, logits = linear(logits_b, 784, 10, activation=None, scope='layer_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder\n",
      "Placeholder_1\n",
      "Placeholder_2\n",
      "layer_1/zeros/shape_as_tensor\n",
      "layer_1/zeros/Const\n",
      "layer_1/zeros\n",
      "layer_1/Variable\n",
      "layer_1/Variable/Assign\n",
      "layer_1/Variable/read\n",
      "layer_1/weights/tag\n",
      "layer_1/weights\n",
      "layer_1/zeros_1\n",
      "layer_1/Variable_1\n",
      "layer_1/Variable_1/Assign\n",
      "layer_1/Variable_1/read\n",
      "layer_1/biases/tag\n",
      "layer_1/biases\n",
      "layer_1/MatMul\n",
      "layer_1/add\n",
      "layer_1/Shape\n",
      "layer_2/zeros/shape_as_tensor\n",
      "layer_2/zeros/Const\n",
      "layer_2/zeros\n",
      "layer_2/Variable\n",
      "layer_2/Variable/Assign\n",
      "layer_2/Variable/read\n",
      "layer_2/weights/tag\n",
      "layer_2/weights\n",
      "layer_2/zeros_1\n",
      "layer_2/Variable_1\n",
      "layer_2/Variable_1/Assign\n",
      "layer_2/Variable_1/read\n",
      "layer_2/biases/tag\n",
      "layer_2/biases\n",
      "layer_2/MatMul\n",
      "layer_2/add\n",
      "layer_2/Shape\n",
      "layer_3/zeros/shape_as_tensor\n",
      "layer_3/zeros/Const\n",
      "layer_3/zeros\n",
      "layer_3/Variable\n",
      "layer_3/Variable/Assign\n",
      "layer_3/Variable/read\n",
      "layer_3/weights/tag\n",
      "layer_3/weights\n",
      "layer_3/zeros_1\n",
      "layer_3/Variable_1\n",
      "layer_3/Variable_1/Assign\n",
      "layer_3/Variable_1/read\n",
      "layer_3/biases/tag\n",
      "layer_3/biases\n",
      "layer_3/MatMul\n",
      "layer_3/add\n",
      "layer_3/Shape\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations(): \n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `logits` is a matrix with `num_images` rows and `num_classes` columns, where the element of the $i$'th row and $j$'th column is an estimate of how likely the $i$'th input image is to be of the $j$'th class.\n",
    "\n",
    "However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, so we want to normalize them so that each row of the `logits` matrix sums to one, and each element is limited between zero and one. This is calculated using the so-called softmax function and the result is stored in `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted class can be calculated from the `y_pred` matrix by taking the index of the largest element in each row.\n",
    "\n",
    "In simpler terms, we are squashing all the numbers down in our vector to less than 1 (normalization), and then choosing the largest value from the number of classes to be the predicted class. \n",
    "\n",
    "For example, if after the softmax function we have a vector of [0.1, 0.07, 0.03, 0.75, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0], then our \"argmax\" function would select 0.75 as the maximum value. This is the 4th element of our 10 classes, and thus we are predicting the input is a 3 (0 is the 1st element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_cls = tf.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-function to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the model better at classifying the input images, we must somehow change the variables for `weights` and `biases`. To do this we first need to know how well the model currently performs by comparing the predicted output of the model `y_pred` to the desired output `y_true`.\n",
    "\n",
    "The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. The goal of optimization is therefore to minimize the cross-entropy so it gets as close to zero as possible by changing the `weights` and `biases` of the model.\n",
    "\n",
    "TensorFlow has a built-in function for calculating the cross-entropy. Note that it uses the values of the `logits` because it also calculates the softmax internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Rank mismatch: Rank of labels (received 2) should equal rank of logits minus 1 (received 2).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c6032526d8f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n\u001b[0;32m----> 2\u001b[0;31m                                                         labels=y_true)\n\u001b[0m",
      "\u001b[0;32m/Users/jonathansherman/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   2038\u001b[0m       raise ValueError(\"Rank mismatch: Rank of labels (received %s) should \"\n\u001b[1;32m   2039\u001b[0m                        \u001b[0;34m\"equal rank of logits minus 1 (received %s).\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m                        (labels_static_shape.ndims, logits.get_shape().ndims))\n\u001b[0m\u001b[1;32m   2041\u001b[0m     if (static_shapes_fully_defined and\n\u001b[1;32m   2042\u001b[0m         labels_static_shape != logits.get_shape()[:-1]):\n",
      "\u001b[0;31mValueError\u001b[0m: Rank mismatch: Rank of labels (received 2) should equal rank of logits minus 1 (received 2)."
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                        labels=y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now calculated the cross-entropy for each of the image classifications so we have a measure of how well the model performs on each image individually. But in order to use the cross-entropy to guide the optimization of the model's variables we need a single scalar value, so we simply take the average of the cross-entropy for all the image classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(cross_entropy)\n",
    "tf.summary.scalar(\"cost\", cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a cost measure that must be minimized, we can then create an optimizer. In this case it is the basic form of Gradient Descent where the step-size is set to 0.5.\n",
    "\n",
    "Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a few more performance measures to display the progress to the user.\n",
    "\n",
    "This is a vector of booleans whether the predicted class equals the true class of each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the classification accuracy by first type-casting the vector of booleans to floats, so that False becomes 0 and True becomes 1, and then calculating the average of these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow session\n",
    "\n",
    "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "## merge all our Tensorboard summaries and create file directory\n",
    "summaryMerged = tf.summary.merge_all()\n",
    "filename=\"./summary_log_qd/run\"+datetime.datetime.now().strftime(\"%Y-%m-%d--%H-%M-%S\")\n",
    "writer = tf.summary.FileWriter(filename, session.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables\n",
    "\n",
    "The variables for `weights` and `biases` must be initialized before we start optimizing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to perform optimization iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There are 50.000 images in the training-set. It takes a long time to calculate the gradient of the model using all these images. We therefore use Stochastic Gradient Descent which only uses a small batch of images in each iteration of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for performing a number of optimization iterations so as to gradually improve the `weights` and `biases` of the model. In each iteration, a new batch of data is selected from the training-set and then TensorFlow executes the optimizer using those training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    x_batch = [] \n",
    "    y_true_batch = []\n",
    "    for i in range(num_iterations):\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.        \n",
    "        \n",
    "        #uncomment and insert if random batch sampling desired\n",
    "        #idxs = np.random.permutation(range(len(xs)))\n",
    "        \n",
    "        n_batches = len(ytrn_data) // batch_size\n",
    "        for batch_i in range(n_batches):\n",
    "            x_batch = xtrn_data[batch_i * batch_size: (batch_i + 1) * batch_size, :]\n",
    "            y_true_batch = ytrn_onehot[batch_i * batch_size: (batch_i + 1) * batch_size, :]\n",
    "            # Put the batch into a dict with the proper names\n",
    "            # for placeholder variables in the TensorFlow graph.\n",
    "            # Note that the placeholder for y_true_cls is not set\n",
    "            # because it is not used during training.\n",
    "            feed_dict_train = {x: x_batch,\n",
    "                               y_true: y_true_batch}\n",
    "\n",
    "            # Run the optimizer using this batch of training data.\n",
    "            # TensorFlow assigns the variables in feed_dict_train\n",
    "            # to the placeholder variables and then runs the optimizer.\n",
    "            _, sumOut = session.run([optimizer,summaryMerged], feed_dict=feed_dict_train)\n",
    "            #sumOut = session.run(summaryMerged, feed_dict=feed_dict_train)\n",
    "        \n",
    "            if i % 10 == 0:\n",
    "                writer.add_summary(sumOut, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-functions to show performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dict with the test-set data to be used as input to the TensorFlow graph. Note that we must use the correct names for the placeholder variables in the TensorFlow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict_test = {x: xtst_data,\n",
    "                  y_true: ytst_onehot,\n",
    "                  y_true_cls: ytst_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for printing the classification accuracy on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_accuracy():\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test)\n",
    "    \n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for printing and plotting the confusion matrix using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_confusion_matrix():\n",
    "    # Get the true classifications for the test-set.\n",
    "    cls_true = ytst_data\n",
    "    \n",
    "    # Get the predicted classifications for the test-set.\n",
    "    cls_pred = session.run(y_pred_cls, feed_dict=feed_dict_test)\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, range(num_classes))\n",
    "    plt.yticks(tick_marks, range(num_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting examples of images from the test-set that have been mis-classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_example_errors():\n",
    "    # Use TensorFlow to get a list of boolean values\n",
    "    # whether each test-image has been correctly classified,\n",
    "    # and a list for the predicted class of each image.\n",
    "    correct, cls_pred = session.run([correct_prediction, y_pred_cls],\n",
    "                                    feed_dict=feed_dict_test)\n",
    "\n",
    "    # Negate the boolean array.\n",
    "    incorrect = (correct == False)\n",
    "    \n",
    "    # Get the images from the test-set that have been\n",
    "    # incorrectly classified.\n",
    "    images = xtst_data[incorrect]\n",
    "    \n",
    "    # Get the predicted classes for those images.\n",
    "    cls_pred = cls_pred[incorrect]\n",
    "\n",
    "    # Get the true classes for those images.\n",
    "    cls_true = ytst_data[incorrect]\n",
    "    \n",
    "    # Plot the first 9 images.\n",
    "    plot_images(images=images[0:9],\n",
    "                cls_true=cls_true[0:9],\n",
    "                cls_pred=cls_pred[0:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to plot the model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting the `weights` of the model. 10 images are plotted, one for each digit that the model is trained to recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_weights():\n",
    "    # Get the values for the weights from the TensorFlow variable.\n",
    "    w = session.run(weights)\n",
    "    \n",
    "    # Get the lowest and highest values for the weights.\n",
    "    # This is used to correct the colour intensity across\n",
    "    # the images so they can be compared with each other.\n",
    "    w_min = np.min(w)\n",
    "    w_max = np.max(w)\n",
    "\n",
    "    # Create figure with 3x4 sub-plots,\n",
    "    # where the last 2 sub-plots are unused.\n",
    "    fig, axes = plt.subplots(3, 4)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Only use the weights for the first 10 sub-plots.\n",
    "        if i<10:\n",
    "            # Get the weights for the i'th digit and reshape it.\n",
    "            # Note that w.shape == (img_size_flat, 10)\n",
    "            image = w[:, i].reshape(img_shape)\n",
    "\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"Weights: {0}\".format(i))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(image, vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "\n",
    "        # Remove ticks from each sub-plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance before any optimization\n",
    "\n",
    "The accuracy on the test-set is 9.8%. This is because the model has only been initialized and not optimized at all, so it always predicts that the image shows a zero digit, as demonstrated in the plot below, and it turns out that 9.8% of the images in the test-set happens to be zero digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance after 1 optimization iteration\n",
    "\n",
    "Already after a single optimization iteration, the model has increased its accuracy on the test-set to 64.4% up from 10.2%. This means that it mis-classifies the images about 4out of 10 times, as demonstrated on a few examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize(num_iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights can also be plotted as shown below. Positive weights are red and negative weights are blue. These weights can be intuitively understood as image-filters.\n",
    "\n",
    "For example, the weights used to determine if an image shows a zero-digit have a positive reaction (red) to an image of a circle, and  have a negative reaction (blue) to images with content in the centre of the circle.\n",
    "\n",
    "Similarly, the weights used to determine if an image shows a one-digit react positively (red) to a vertical line in the centre of the image, and react negatively (blue) to images with content surrounding that line.\n",
    "\n",
    "Note that the weights mostly look like the digits they're supposed to recognize. This is because only one optimization iteration has been performed so the weights are only trained on 100 images. After training on several thousand images, the weights become more difficult to interpret because they have to recognize many variations of how digits can be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance after 10 optimization iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already performed 1 iteration.\n",
    "optimize(num_iterations=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_errors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance after 100 optimization iterations\n",
    "\n",
    "After 100 optimization iterations, the model only mis-classifies about one in ten images. As demonstrated below, some of the mis-classifications are justified because the images are very hard to determine with certainty even for humans, while others are quite obvious and should have been classified correctly by a good model. But this simple model cannot reach much better performance and more complex models are therefore needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have already performed 10 iterations.\n",
    "optimize(num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_example_errors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has now been trained for 100 optimization iterations, with each iteration using 100 images from the training-set. Because of the great variety of the images, the weights have now become difficult to interpret and we may doubt whether the model truly understands how digits are composed from lines, or whether the model has just memorized many different variations of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print and plot the so-called confusion matrix which lets us see more details about the mis-classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now done using TensorFlow, so we close the session to release its resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This has been commented out in case you want to modify and experiment\n",
    "# with the Notebook without having to restart it.\n",
    "#session.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "These are a few suggestions for exercises that may help improve your skills with TensorFlow. It is important to get hands-on experience with TensorFlow in order to learn how to use it properly.\n",
    "\n",
    "You may want to backup this Notebook before making any changes.\n",
    "\n",
    "* Change the learning-rate for the optimizer.\n",
    "* Change the optimizer to e.g. `AdagradOptimizer` or `AdamOptimizer`.\n",
    "* Change the batch-size to e.g. 1 or 1000.\n",
    "* How do these changes affect the performance?\n",
    "* Do you think these changes will have the same effect (if any) on other classification problems and mathematical models?\n",
    "* Do you get the exact same results if you run the Notebook multiple times without changing any parameters? Why or why not?\n",
    "* Change the function `plot_example_errors()` so it also prints the `logits` and `y_pred` values for the mis-classified examples.\n",
    "* Use `sparse_softmax_cross_entropy_with_logits` instead of `softmax_cross_entropy_with_logits`. This may require several changes to multiple places in the source-code. Discuss the advantages and disadvantages of using the two methods.\n",
    "* Remake the program yourself without looking too much at this source-code.\n",
    "* Explain to a friend how the program works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## License (MIT)\n",
    "\n",
    "Copyright (c) 2016 by [Magnus Erik Hvass Pedersen](http://www.hvass-labs.org/)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
